---
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otel-collector
spec:
  mode: daemonset
  image: otel/opentelemetry-collector-contrib:0.103.1
  volumeMounts:
    - name: pods
      mountPath: /var/log/pods
      readOnly: true
    - name: scratchdir
      mountPath: /var/lib/otelcol
  volumes:
    - name: scratchdir
      emptyDir:
        sizeLimit: 10Gi
    - name: pods
      hostPath:
        path: /var/log/pods
  resources:
    limits:
      cpu: 200m
      memory: 500Mi
  podSecurityContext:
    runAsUser: 0
    runAsNonRoot: false
    runAsGroup: 0
  serviceAccount: collector
  env:
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: K8S_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
  config:
    receivers:
      filelog:
        include:
          - /var/log/pods/*/*/*.log
        exclude:
          # Exclude logs from all containers named otel-collector
          - /var/log/pods/*/otel-collector/*.log
        include_file_name: false
        include_file_path: true
        operators:
          - id: get-format
            routes:
              # this is a hack because of some flux mechanism
              # the actual expression evaluates to "^\\{"
              - expr: body matches "^\\{"
                output: parser-docker
              - expr: body matches "^[^ Z]+ "
                output: parser-crio
              - expr: body matches "^[^ Z]+Z"
                output: parser-containerd
            type: router
          - id: parser-crio
            regex: ^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
            timestamp:
              layout: 2006-01-02T15:04:05.999999999Z07:00
              layout_type: gotime
              parse_from: attributes.time
            type: regex_parser
          - combine_field: attributes.log
            combine_with: ""
            id: crio-recombine
            is_last_entry: attributes.logtag == 'F'
            max_log_size: 102400
            output: extract_metadata_from_filepath
            source_identifier: attributes["log.file.path"]
            type: recombine
          - id: parser-containerd
            regex: ^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
            timestamp:
              layout: "%Y-%m-%dT%H:%M:%S.%LZ"
              parse_from: attributes.time
            type: regex_parser
          - combine_field: attributes.log
            combine_with: ""
            id: containerd-recombine
            is_last_entry: attributes.logtag == 'F'
            max_log_size: 102400
            output: extract_metadata_from_filepath
            source_identifier: attributes["log.file.path"]
            type: recombine
          - id: parser-docker
            output: extract_metadata_from_filepath
            timestamp:
              layout: "%Y-%m-%dT%H:%M:%S.%LZ"
              parse_from: attributes.time
            type: json_parser
          - id: extract_metadata_from_filepath
            parse_from: attributes["log.file.path"]
            regex: ^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]+)\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$
            type: regex_parser
          - from: attributes.stream
            to: attributes["log.iostream"]
            type: move
          - from: attributes.container_name
            to: resource["k8s.container.name"]
            type: move
          - from: attributes.namespace
            to: resource["k8s.namespace.name"]
            type: move
          - from: attributes.pod_name
            to: resource["k8s.pod.name"]
            type: move
          - from: attributes.restart_count
            to: resource["k8s.container.restart_count"]
            type: move
          - from: attributes.uid
            to: resource["k8s.pod.uid"]
            type: move
          - from: attributes.log
            to: body
            type: move
          - type: add
            field: resource["cluster"]
            value: "main"
          - type: add
            field: resource["job"]
            value: "pod-logging"
        retry_on_failure:
          enabled: true
        start_at: end

    processors:
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25

      batch:
        send_batch_size: 8192
        timeout: 200ms
        send_batch_max_size: 0

      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        filter:
          node_from_env_var: K8S_NODE_NAME
        extract:
          metadata:
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
            - container.id
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: connection
      resource:
        attributes:
          - action: insert
            key: loki.resource.labels
            value: k8s.pod.uid, k8s.pod.ip, k8s.pod.name, k8s.deployment.name, k8s.namespace.name, cluster, job, k8s.container.name, k8s.node.name

          - action: insert
            key: loki.format
            value: raw

    extensions:
      health_check:
        endpoint: ${env:K8S_POD_IP}:13133

      file_storage:
        directory: /var/lib/otelcol
        timeout: 1s
        compaction:
          on_start: true
          on_rebound: true
          directory: /var/lib/otelcol
          max_transaction_size: 65530

      memory_ballast:
        size_in_percentage: 80

    exporters:
      debug:
        verbosity: detailed
      loki:
        endpoint: "http://loki-write.observability.svc.cluster.local/loki/api/v1/push"
        retry_on_failure:
          initial_interval: 5s
          max_interval: 15s
          max_elapsed_time: 30s
        sending_queue:
          enabled: true
          storage: file_storage
          num_consumers: 5
          queue_size: 300
    service:
      extensions:
        - health_check
        - file_storage
        - memory_ballast
      pipelines:
        logs:
          receivers:
            - filelog
          processors:
            - resource
            - batch
          exporters:
            - loki
      telemetry:
        metrics:
          level: detailed
          address: 0.0.0.0:8888
        logs:
          level: "INFO"
          development: false
